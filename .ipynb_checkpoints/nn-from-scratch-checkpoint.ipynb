{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd2bdb4",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "390a3c1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.381079Z",
     "start_time": "2023-01-22T19:41:28.375780Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "acc38886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.387745Z",
     "start_time": "2023-01-22T19:41:28.384749Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "4cb3db1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.395682Z",
     "start_time": "2023-01-22T19:41:28.390615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7F7B9A636240"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79c42e",
   "metadata": {},
   "source": [
    "## Single Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b1242c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.400721Z",
     "start_time": "2023-01-22T19:41:28.398178Z"
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        return self.weight * a + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "8c6832a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.404807Z",
     "start_time": "2023-01-22T19:41:28.401892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(3, 10)\n",
    "p.feedforward(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5550d",
   "metadata": {},
   "source": [
    "## Single Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "c9e2abdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.408008Z",
     "start_time": "2023-01-22T19:41:28.405978Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "e121be8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.411262Z",
     "start_time": "2023-01-22T19:41:28.408918Z"
    }
   },
   "outputs": [],
   "source": [
    "class SigmoidNeuron(object):\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        z = self.weight * a + self.bias\n",
    "        return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "515ae277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.473405Z",
     "start_time": "2023-01-22T19:41:28.412250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7b8894ed90>"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = SigmoidNeuron(1, 0)\n",
    "\n",
    "S = []\n",
    "for i in range(-10, 10):\n",
    "    S.append(s.feedforward(i))\n",
    "    \n",
    "plt.scatter(x = range(-10, 10), y = S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592d29a",
   "metadata": {},
   "source": [
    "## Simple Neural Network Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "c855c00f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.477453Z",
     "start_time": "2023-01-22T19:41:28.474298Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    def __init__(self, shapes):\n",
    "        self.n_layers = len(shapes)\n",
    "        self.shapes = shapes\n",
    "        \n",
    "        # initialise weights and biases randomly\n",
    "        self.weights = [np.random.randn(n_neuron, previous_n_neuron) for previous_n_neuron, n_neuron in zip(shapes[:-1], shapes[1:])]\n",
    "        self.biases = [np.random.randn(n_neuron, 1) for n_neuron in shapes[1:]]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = sigmoid(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea72cb",
   "metadata": {},
   "source": [
    "### Initialising a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "739b141a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.482882Z",
     "start_time": "2023-01-22T19:41:28.479681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simple network has 4 layers.\n",
      "\n",
      "The network's weights matrix by layer: \n",
      "[[-0.89504241 -0.73744706 -0.57335804  1.54682996  1.01803026]\n",
      " [-1.35523036  1.50832384 -1.50159842 -0.43310977 -0.72361145]\n",
      " [-1.2652303   1.83552196 -0.9491905   0.78455285 -0.10049512]]\n",
      "[[-0.91719061 -0.62485054 -0.54340994]\n",
      " [ 0.29368888  0.74763007 -0.17596795]]\n",
      "[[0.74152067 0.16362396]]\n",
      "\n",
      "The network's biases matrix by layer: \n",
      "[[ 0.39136402]\n",
      " [-1.04063754]\n",
      " [ 1.59666857]]\n",
      "[[ 1.7157512 ]\n",
      " [-0.91933673]]\n",
      "[[-0.51556119]]\n"
     ]
    }
   ],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "sn = SimpleNetwork(shapes)\n",
    "\n",
    "print(\"The simple network has {} layers.\".format(sn.n_layers), end = '\\n\\n')\n",
    "print(\"The network's weights matrix by layer: \\n\" + \n",
    "            \"\\n\".join(\"{}\".format(w) for w in sn.weights), end = '\\n\\n')\n",
    "print(\"The network's biases matrix by layer: \\n\" + \n",
    "            \"\\n\".join(\"{}\".format(b) for b in sn.biases) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c9b2b",
   "metadata": {},
   "source": [
    "There should be 3 matrices for each of weights and biases, as each non-input layer has a weight and bias matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350b701",
   "metadata": {},
   "source": [
    "The weight matrix of a non-input layer is of shape __(j x k)__, where:  \n",
    "__j__ = number of nodes in this _current_ layer (start at non-input layer)  \n",
    "__k__ = number of nodes in the _previous_ layer (end at non-output layer)  \n",
    "\n",
    "This in turn agrees with our notation for a specific weight variable, __$w^l_jk$__ , connecting the kth neuron from the previous layer to the jth neuron in the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82f8b3",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "bfed86eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.486266Z",
     "start_time": "2023-01-22T19:41:28.483686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48823096]])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[32], [12], [11], [88], [47]]) # 5x1 input\n",
    "\n",
    "sn.feedforward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a2a6b",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456474a1",
   "metadata": {},
   "source": [
    "The aim of the neural network is to minimise the difference bettween desired and actual prediction from a given set of inputs. This difference is measured using a cost function, and it can be shown that the discrepency of the data is directly proportional to the error, by using chain-rule.\n",
    "- The backpropagation algorithm is for calculating partial derivatives of Cost with respect to weights, biases and weighted activation (aka the error). \n",
    "- The error is defined as __partial derivative of cost, C,  with respect to the weighted activation, z__.\n",
    "- We assume using the quadratic cost function here, where $C = \\frac{1}{2n} \\sum_x (y(x) - a)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086dbe7",
   "metadata": {},
   "source": [
    "### A brief reminder of the four equations used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40e3c3",
   "metadata": {},
   "source": [
    "1. Error in the output layer  \n",
    "    $\\delta ^L = \\nabla _a C \\odot \\sigma ' (z^L)$\n",
    "1. Error of previous layer in terms of current layer (computing backward)  \n",
    "    $\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^l)$\n",
    "1. Rate of change of cost with respect to bias, in terms of error  \n",
    "    $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$\n",
    "1. Rate of change of cost with respect to weight, in terms of error  \n",
    "    $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$\n",
    "   \n",
    "__*note that in the code, error is named 'delta', as this is the Greek letter used to represent error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "90b5734a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.489847Z",
     "start_time": "2023-01-22T19:41:28.487611Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# this is the partial derivative of cost w.r.t. output activation, assuming quadratic cost function is used\n",
    "def cost_derivative(output_activations, y): \n",
    "    return (output_activations - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "aaf4aa1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.494887Z",
     "start_time": "2023-01-22T19:41:28.490657Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inherit the SimpleNetwork class and add the backpropagation algorithm\n",
    "class Network(SimpleNetwork):\n",
    "    def backprop(self, x, y):\n",
    "        # create empty arrays for storing the partial derivatives \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # first, gather each neuron's weighted activation and activation by layer\n",
    "        a = x\n",
    "        A = [x]\n",
    "        Z = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)\n",
    "            A.append(a)\n",
    "        # uncomment to see the weighted activation and activation matrices\n",
    "        # print(\"Weighted activations for each non-input layer are: \\n\" + \n",
    "        #              \"\\n\".join(\"{}\".format(z) for z in Z), end = '\\n\\n')\n",
    "        # print(\"Activations for each non-input layer are: \\n\" + \n",
    "        #              \"\\n\".join(\"{}\".format(a) for a in A), end = '\\n\\n')\n",
    "        \n",
    "        # now computes the error for each neuron, by layer in reversing order\n",
    "        delta = cost_derivative(A[-1], y) * sigmoid_prime(Z[-1]) # equation 1 ('*' operation is Hadamard product)\n",
    "        nabla_b[-1] = delta # equation 3\n",
    "        nabla_w[-1] = np.dot(delta, A[-2].transpose()) # equation 4\n",
    "        \n",
    "        for i in range(2, self.n_layers):\n",
    "            z = Z[-i]\n",
    "            delta = np.dot(self.weights[-i+1].transpose(), delta) * sigmoid_prime(z) # equation 2\n",
    "            nabla_b[-i] = delta # equation 3\n",
    "            nabla_w[-i] = np.dot(delta, A[-i-1].transpose()) # equation 4\n",
    "        \n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732a509",
   "metadata": {},
   "source": [
    "### Initialising network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "b33ce03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.497402Z",
     "start_time": "2023-01-22T19:41:28.495709Z"
    }
   },
   "outputs": [],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "n = Network(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022c78f",
   "metadata": {},
   "source": [
    "### Calculate partial derivatives with back propagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "98c9baf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.500203Z",
     "start_time": "2023-01-22T19:41:28.498146Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[32], [12], [11], [88], [47]]) # 5x1 input\n",
    "y = np.array([[0.7]]) # 1x1 output\n",
    "\n",
    "nabla_b, nabla_w = n.backprop(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "3c2fe0f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.503227Z",
     "start_time": "2023-01-22T19:41:28.500941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of partial C/partial b matrix by layer:  \n",
      " [(3, 1), (2, 1), (1, 1)]\n",
      "\n",
      "shape of partial C/partial w matrix by layer:  \n",
      " [(3, 5), (2, 3), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "n_b_shape = [nabla_b[i].shape for i in range(len(nabla_b))]\n",
    "n_w_shape = [nabla_w[i].shape for i in range(len(nabla_w))]\n",
    "\n",
    "print('shape of partial C/partial b matrix by layer: ', '\\n', n_b_shape, end = '\\n\\n')\n",
    "print('shape of partial C/partial w matrix by layer: ', '\\n', n_w_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "dd58acff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.506333Z",
     "start_time": "2023-01-22T19:41:28.504900Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(nabla_b)\n",
    "# print(nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabbb9f9",
   "metadata": {},
   "source": [
    "## Train the Neural Network with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612632fc",
   "metadata": {},
   "source": [
    "With the back-propagation algorithm, we can compute the partial derivatives of cost with respect to weights and biases. Now we can apply gradient descent by adjusting weights and biases after each training input for the neural network to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dce87a",
   "metadata": {},
   "source": [
    "To do this, for any weight or bias parameters $v$, we choose:  $\\Delta v =  - \\frac{1}{\\eta} \\frac{\\partial C}{\\partial v}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "f449f3bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.510951Z",
     "start_time": "2023-01-22T19:41:28.507199Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inherit the Network class and add the gradient descent algorithm\n",
    "class TrainNetwork(Network):  \n",
    "    def gradient_descent(self, training_data, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # compute the sum of gradient matrices with the entire training data\n",
    "        for x, y in training_data:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # update parameters\n",
    "        self.weights = [w - (eta/len(training_data))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(training_data))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def train_1_epoch(self, training_data, eta):\n",
    "        for x, y in training_data: # training_data should be in the form of key-value pairs\n",
    "            self.gradient_descent(training_data, eta)\n",
    "    \n",
    "    def predict(self, testing_data):\n",
    "        for x, y in testing_data: # testing_data should be in the form of key-value pairs\n",
    "            print(self.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb5246",
   "metadata": {},
   "source": [
    "As an example, let's assume we want the network to always predict 1, regardless of the input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d234",
   "metadata": {},
   "source": [
    "### Initialising network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "b95f1941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.513239Z",
     "start_time": "2023-01-22T19:41:28.511689Z"
    }
   },
   "outputs": [],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "tn = TrainNetwork(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d89cc",
   "metadata": {},
   "source": [
    "### Initial network prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "e9894fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.517172Z",
     "start_time": "2023-01-22T19:41:28.513955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47453268]]\n",
      "[[0.48386753]]\n",
      "[[0.48661441]]\n",
      "[[0.47755661]]\n",
      "[[0.47444594]]\n",
      "[[0.47743556]]\n",
      "[[0.48432643]]\n",
      "[[0.47356207]]\n",
      "[[0.49634058]]\n",
      "[[0.47383789]]\n"
     ]
    }
   ],
   "source": [
    "x = [np.random.randn(5, 1) for i in range(10)]\n",
    "y = [np.ones(shape = (1, 1)) for i in range(10)]\n",
    "testing_data = list(zip(x, y))\n",
    "\n",
    "tn.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f392b",
   "metadata": {},
   "source": [
    "### Training neural network with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "e62a4218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.946319Z",
     "start_time": "2023-01-22T19:41:28.517866Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [np.random.randn(5, 1) for i in range(100)]\n",
    "y = [np.ones(shape = (1, 1)) for i in range(100)]\n",
    "training_data = list(zip(x, y))\n",
    "\n",
    "tn.train_1_epoch(training_data, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fb8fe",
   "metadata": {},
   "source": [
    "### Predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "aaa7bfb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.951931Z",
     "start_time": "2023-01-22T19:41:28.947392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92002388]]\n",
      "[[0.91200459]]\n",
      "[[0.91974072]]\n",
      "[[0.90776076]]\n",
      "[[0.90532923]]\n",
      "[[0.91216285]]\n",
      "[[0.92647399]]\n",
      "[[0.90556839]]\n",
      "[[0.90367154]]\n",
      "[[0.91177097]]\n"
     ]
    }
   ],
   "source": [
    "# x = [np.random.randn(5, 1) for i in range(10)]\n",
    "# y = [np.ones(shape = (1, 1)) for i in range(10)]\n",
    "# testing_data = zip(x, y)\n",
    "\n",
    "tn.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83667c68",
   "metadata": {},
   "source": [
    "## Introducing Stochastic Gradient Descent with Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80008e6",
   "metadata": {},
   "source": [
    "Using stochastic gradient descent, we split the training data into k mini-batches to speed up the training process.  \n",
    "For each mini batch, we compute the sum of the gradients for weights and biases:\n",
    "- nabla_b and nabla_w are the sum of delta_nabla_b and delta_nabla_w  \n",
    "- delta_nabla_b and delta_nabla_w are the gradients calculated by an entry of data in the mini batch  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cde326",
   "metadata": {},
   "source": [
    "The neural network's weights and biases are updated for each mini-batch. Now for each epoch, we would have updated the network's parameters k times, as we have split the data into k mini batches.\n",
    "\n",
    "Assuming that the randomly chosen mini batches are representative of the entire training data, by updating the parameters more frequently we reduces the time taken for the model to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "14000c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.958522Z",
     "start_time": "2023-01-22T19:41:28.953019Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inherit the Network class and add the stochastic gradient descent algorithm\n",
    "class SGDTrainNetwork(Network):\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # create empty arrays for storing the partial derivatives \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # compute the sum of gradient matrices with the entire mini batch\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # update parameters using the averaged gradient matrices\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, testing_data = None):\n",
    "        n_train = len(training_data)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n_train, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if testing_data:\n",
    "                n_test = len(testing_data)\n",
    "                print('Epoch {0}: prediction accuracy = {1}/{2}'.format(i, self.evaluate(testing_data), n_test))\n",
    "            else:\n",
    "                print('Epoch {0} complete.'.format(i))\n",
    "    \n",
    "    def evaluate(self, testing_data):\n",
    "        results = [(np.rint(self.feedforward(x)), y) for x, y in testing_data]\n",
    "        return sum(int(x == y) for x, y in results)\n",
    "    \n",
    "    def predict(self, testing_data):\n",
    "        for x, y in testing_data: # testing_data should be in the form of key-value pairs\n",
    "            print(self.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6317e1",
   "metadata": {},
   "source": [
    "### Initialising network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "ab41cfa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.961034Z",
     "start_time": "2023-01-22T19:41:28.959197Z"
    }
   },
   "outputs": [],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "sgdtn = SGDTrainNetwork(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55b2c4",
   "metadata": {},
   "source": [
    "### Initial network prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "49c5d989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.970995Z",
     "start_time": "2023-01-22T19:41:28.961848Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'zip' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [461]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# x = [np.random.randn(5, 1) for i in range(100)]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# y = [np.ones(shape = (1, 1)) for i in range(100)]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# testing_data = list(zip(x, y))\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sgdtn\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mtesting_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'zip' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# x = [np.random.randn(5, 1) for i in range(100)]\n",
    "# y = [np.ones(shape = (1, 1)) for i in range(100)]\n",
    "# testing_data = list(zip(x, y))\n",
    "\n",
    "sgdtn.predict(testing_data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e00a0",
   "metadata": {},
   "source": [
    "### Training neural network with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fa4be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.971849Z",
     "start_time": "2023-01-22T19:41:28.971843Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = [np.random.randn(5, 1) for i in range(100)]\n",
    "# y = [np.ones(shape = (1, 1)) for i in range(100)]\n",
    "# training_data = list(zip(x, y))\n",
    "\n",
    "sgdtn.SGD(training_data, 10, 20, 0.5, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e4bbc",
   "metadata": {},
   "source": [
    "### Predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccb034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T19:41:28.972370Z",
     "start_time": "2023-01-22T19:41:28.972364Z"
    }
   },
   "outputs": [],
   "source": [
    "sgdtn.predict(testing_data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a952c",
   "metadata": {},
   "source": [
    "## Some Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fec57",
   "metadata": {},
   "source": [
    "### GD vs SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46852d84",
   "metadata": {},
   "source": [
    "In both gradient descent (GD) and stochastic gradient descent (SGD), we update the parameters iteratively to minimize the cost function.\n",
    "- In GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration\n",
    "- In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d70e08",
   "metadata": {},
   "source": [
    "Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.\n",
    "\n",
    "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b4a6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T14:12:01.228670Z",
     "start_time": "2022-08-30T14:12:01.224120Z"
    }
   },
   "source": [
    "### Choice of hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d438e2e",
   "metadata": {},
   "source": [
    "### Complexity & Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bdf334",
   "metadata": {},
   "source": [
    "## Learning with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c1200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
