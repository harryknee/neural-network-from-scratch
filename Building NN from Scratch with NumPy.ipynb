{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd2bdb4",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "390a3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc38886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79c42e",
   "metadata": {},
   "source": [
    "## Single Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1242c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        return self.weight * a + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c6832a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(3, 10)\n",
    "p.feedforward(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5550d",
   "metadata": {},
   "source": [
    "## Single Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9e2abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e121be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidNeuron(object):\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        z = self.weight * a + self.bias\n",
    "        return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "515ae277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd7cadc2580>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = SigmoidNeuron(1, 0)\n",
    "\n",
    "S = []\n",
    "for i in range(-10, 10):\n",
    "    S.append(s.feedforward(i))\n",
    "    \n",
    "plt.scatter(x = range(-10, 10), y = S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592d29a",
   "metadata": {},
   "source": [
    "## Simple Neural Network Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c855c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    def __init__(self, shapes):\n",
    "        self.n_layers = len(shapes)\n",
    "        self.shapes = shapes\n",
    "        \n",
    "        # initialise weights and biases randomly\n",
    "        self.weights = [np.random.randn(n_neuron, previous_n_neuron) for previous_n_neuron, n_neuron in zip(shapes[:-1], shapes[1:])]\n",
    "        self.biases = [np.random.randn(n_neuron, 1) for n_neuron in shapes[1:]]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = sigmoid(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea72cb",
   "metadata": {},
   "source": [
    "### Initialising a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "739b141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simple network has 4 layers.\n",
      "\n",
      "The network's weights matrix by layer: \n",
      "[[ 1.22539989  0.94858873 -0.10298879 -1.28347253  0.84358115]\n",
      " [ 0.96231907 -1.34075306  1.02863004 -0.38746999 -0.09440164]\n",
      " [ 0.29982213 -0.65610801  0.48503218  0.53749706  0.4190548 ]]\n",
      "[[ 0.71827974  1.10068235 -1.73542741]\n",
      " [ 0.51085526 -0.23204873  0.60853647]]\n",
      "[[0.04432175 1.11654027]]\n",
      "\n",
      "The network's biases matrix by layer: \n",
      "[[-0.44320917]\n",
      " [ 0.35085479]\n",
      " [ 0.98115904]]\n",
      "[[ 0.22433538]\n",
      " [-0.38916931]]\n",
      "[[0.3652716]]\n"
     ]
    }
   ],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "sn = SimpleNetwork(shapes)\n",
    "\n",
    "print(\"The simple network has {} layers.\".format(sn.n_layers), end = '\\n\\n')\n",
    "print(\"The network's weights matrix by layer: \\n\" + \n",
    "            \"\\n\".join(\"{}\".format(w) for w in sn.weights), end = '\\n\\n')\n",
    "print(\"The network's biases matrix by layer: \\n\" + \n",
    "            \"\\n\".join(\"{}\".format(b) for b in sn.biases) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c9b2b",
   "metadata": {},
   "source": [
    "There should be 3 matrices for each of weights and biases, as each non-input layer has a weight and bias matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350b701",
   "metadata": {},
   "source": [
    "The weight matrix of a non-input layer is of shape __(j x k)__, where:  \n",
    "__j__ = number of nodes in this _current_ layer (start at non-input layer)  \n",
    "__k__ = number of nodes in the _previous_ layer (end at non-output layer)  \n",
    "\n",
    "This in turn agrees with our notation for a specific weight variable, __$w^l_jk$__ , connecting the kth neuron from the previous layer to the jth neuron in the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82f8b3",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfed86eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72958982]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[32], [12], [11], [88], [47]]) # 5x1 input\n",
    "\n",
    "sn.feedforward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a2a6b",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456474a1",
   "metadata": {},
   "source": [
    "The aim of the neural network is to minimise the difference bettween desired and actual prediction from a given set of inputs. This difference is measured using a cost function, and it can be shown that the discrepency of the data is directly proportional to the error, by using chain-rule.\n",
    "- The backpropagation algorithm is for calculating partial derivatives of Cost with respect to weights, biases and weighted activation (aka the error). \n",
    "- The error is defined as __partial derivative of cost, C,  with respect to the weighted activation, z__.\n",
    "- We assume using the quadratic cost function here, where __C = sum(0.5 * (y - a)^2)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086dbe7",
   "metadata": {},
   "source": [
    "### A brief reminder of the four equations used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40e3c3",
   "metadata": {},
   "source": [
    "1. Error in the output layer  \n",
    "    $\\delta ^L = \\nabla _a C \\odot \\sigma ' (z^L)$\n",
    "1. Error of previous layer in terms of current layer (computing backward)  \n",
    "    $\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^l)$\n",
    "1. Rate of change of cost with respect to bias, in terms of error  \n",
    "    $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$\n",
    "1. Rate of change of cost with respect to weight, in terms of error  \n",
    "    $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$\n",
    "   \n",
    "__*note that in the code, error is named 'delta', as this is the Greek letter used to represent error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90b5734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# this is the partial derivative of cost w.r.t. output activation, assuming quadratic cost function is used\n",
    "def cost_derivative(output_activations, y): \n",
    "    return (output_activations - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaf4aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we inherit the SimpleNetwork class and add the backpropagation algorithm\n",
    "class Network(SimpleNetwork):\n",
    "    def backprop(self, x, y):\n",
    "        # create empty arrays for storing the partial derivatives \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # first, gather each neuron's weighted activation and activation by layer\n",
    "        a = x\n",
    "        A = [x]\n",
    "        Z = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)\n",
    "            A.append(a)\n",
    "        # uncomment to see the weighted activation and activation matrices\n",
    "        # print(\"Weighted activations for each non-input layer are: \\n\" + \n",
    "        #              \"\\n\".join(\"{}\".format(z) for z in Z), end = '\\n\\n')\n",
    "        # print(\"Activations for each non-input layer are: \\n\" + \n",
    "        #              \"\\n\".join(\"{}\".format(a) for a in A), end = '\\n\\n')\n",
    "        \n",
    "        # now computes the error for each neuron, by layer in reversing order\n",
    "        delta = cost_derivative(A[-1], y) * sigmoid_prime(Z[-1]) # equation 1 (numpy's '*' operation is Hadamard product)\n",
    "        nabla_b[-1] = delta # equation 3\n",
    "        nabla_w[-1] = np.dot(delta, A[-2].transpose()) # equation 4\n",
    "        \n",
    "        for i in range(2, self.n_layers):\n",
    "            z = Z[-i]\n",
    "            delta = np.dot(self.weights[-i+1].transpose(), delta) * sigmoid_prime(z) # equation 2\n",
    "            nabla_b[-i] = delta # equation 3\n",
    "            nabla_w[-i] = np.dot(delta, A[-i-1].transpose()) # equation 4\n",
    "        \n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732a509",
   "metadata": {},
   "source": [
    "### Initialising network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b33ce03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "n = Network(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022c78f",
   "metadata": {},
   "source": [
    "### Calculate partial derivatives with back propagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c9baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[32], [12], [11], [88], [47]]) # 5x1 input\n",
    "y = np.array([[0.7]]) # 1x1 output\n",
    "\n",
    "nabla_b, nabla_w = n.backprop(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c2fe0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of partial C/partial b matrix by layer:  \n",
      " [(3, 1), (2, 1), (1, 1)]\n",
      "\n",
      "shape of partial C/partial w matrix by layer:  \n",
      " [(3, 5), (2, 3), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "n_b_shape = [nabla_b[i].shape for i in range(len(nabla_b))]\n",
    "n_w_shape = [nabla_w[i].shape for i in range(len(nabla_w))]\n",
    "\n",
    "print('shape of partial C/partial b matrix by layer: ', '\\n', n_b_shape, end = '\\n\\n')\n",
    "print('shape of partial C/partial w matrix by layer: ', '\\n', n_w_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd58acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nabla_b)\n",
    "# print(nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabbb9f9",
   "metadata": {},
   "source": [
    "## Train the Neural Network with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612632fc",
   "metadata": {},
   "source": [
    "With the back-propagation algorithm, we can compute the partial derivatives of cost with respect to weights and biases. Now we can apply gradient descent by adjusting weights and biases after each training input for the neural network to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dce87a",
   "metadata": {},
   "source": [
    "To do this, for any weight or bias parameters $v$, we choose:  $\\Delta v =  - \\frac{1}{\\eta} \\frac{\\partial C}{\\partial v}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f449f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we inherit the Network class and add the gradient descent algorithm\n",
    "class TrainNetwork(Network):  \n",
    "    def gradient_descent(self, x, y, eta):\n",
    "        nabla_b, nabla_w = self.backprop(x, y)\n",
    "        \n",
    "        self.weights = [w - eta*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - eta*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def train(self, training_data, eta):\n",
    "        for x, y in training_data: # training_data should be in the form of key-value pairs\n",
    "            self.gradient_descent(x, y, eta)\n",
    "    \n",
    "    def predict(self, testing_data):\n",
    "        for x, y in testing_data: # testing_data should be in the form of key-value pairs\n",
    "            print(self.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb5246",
   "metadata": {},
   "source": [
    "As an example, let's assume we want the network to always predict 1, regardless of the input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d234",
   "metadata": {},
   "source": [
    "### Initialising network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b95f1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [5, 3, 2, 1]\n",
    "tn = TrainNetwork(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d89cc",
   "metadata": {},
   "source": [
    "### Initial network prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9894fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67535184]]\n",
      "[[0.66975101]]\n",
      "[[0.66898144]]\n",
      "[[0.66857881]]\n",
      "[[0.66757658]]\n",
      "[[0.66206644]]\n",
      "[[0.67849347]]\n",
      "[[0.67047094]]\n",
      "[[0.6690164]]\n",
      "[[0.66982682]]\n"
     ]
    }
   ],
   "source": [
    "x = [np.random.randn(5, 1) for i in range(10)]\n",
    "y = [np.ones(shape = (1, 1)) for i in range(10)]\n",
    "testing_data = zip(x, y)\n",
    "\n",
    "tn.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f392b",
   "metadata": {},
   "source": [
    "### Training neural network with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e62a4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.random.randn(5, 1) for i in range(100)]\n",
    "y = [np.ones(shape = (1, 1)) for i in range(100)]\n",
    "training_data = zip(x, y)\n",
    "\n",
    "tn.train(training_data, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fb8fe",
   "metadata": {},
   "source": [
    "### Predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaa7bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91052339]]\n",
      "[[0.91187056]]\n",
      "[[0.91036278]]\n",
      "[[0.90980336]]\n",
      "[[0.91097329]]\n",
      "[[0.91339744]]\n",
      "[[0.9096447]]\n",
      "[[0.90990799]]\n",
      "[[0.91311995]]\n",
      "[[0.91286515]]\n"
     ]
    }
   ],
   "source": [
    "x = [np.random.randn(5, 1) for i in range(10)]\n",
    "y = [np.ones(shape = (1, 1)) for i in range(10)]\n",
    "testing_data = zip(x, y)\n",
    "\n",
    "tn.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83667c68",
   "metadata": {},
   "source": [
    "## Introducing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80008e6",
   "metadata": {},
   "source": [
    "With stochastic gradient descent, we randomly split the training data into mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14000c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we inherit the Network class and add the gradient descent algorithm\n",
    "class SGDTrainNetwork(Network):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
